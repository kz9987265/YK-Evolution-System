"""
æ··åˆ AI ç³»çµ± - Qwen3 æœ¬åœ°æ¨¡å‹ + Gemini API
æ™ºèƒ½è·¯ç”±ï¼šæ ¹æ“šä»»å‹™è¤‡é›œåº¦é¸æ“‡æœ€ä½³æ¨ç†å¼•æ“
"""

import os
from pathlib import Path
from typing import Optional, Dict, Any, List
import json
from datetime import datetime

try:
    from llama_cpp import Llama
    LLAMA_AVAILABLE = True
except ImportError:
    LLAMA_AVAILABLE = False
    print("âš ï¸  llama-cpp-python not installed. Install with: pip install llama-cpp-python")

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("âš ï¸  google-generativeai not installed. Install with: pip install google-generativeai")


class HybridAI:
    """æ··åˆ AI æ¨ç†ç³»çµ±"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.llm_path = project_root / "LLM"
        
        # åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹
        self.local_model: Optional[Llama] = None
        self.gemini_model = None
        
        # æ€§èƒ½çµ±è¨ˆ
        self.stats = {
            "local_calls": 0,
            "gemini_calls": 0,
            "local_tokens": 0,
            "gemini_tokens": 0,
            "avg_local_time": 0,
            "avg_gemini_time": 0
        }
        
        self._initialize_models()
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
        # 1. åˆå§‹åŒ–æœ¬åœ° Qwen3 æ¨¡å‹
        if LLAMA_AVAILABLE:
            model_file = self._find_gguf_model()
            if model_file:
                try:
                    print(f"ğŸ”„ åŠ è¼‰æœ¬åœ°æ¨¡å‹: {model_file.name}")
                    self.local_model = Llama(
                        model_path=str(model_file),
                        n_ctx=8192,  # ä¸Šä¸‹æ–‡çª—å£
                        n_gpu_layers=-1,  # ä½¿ç”¨ GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        n_threads=8,  # CPU ç·šç¨‹æ•¸
                        verbose=False
                    )
                    print("âœ… æœ¬åœ°æ¨¡å‹åŠ è¼‰æˆåŠŸï¼")
                except Exception as e:
                    print(f"âŒ æœ¬åœ°æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
        
        # 2. åˆå§‹åŒ– Gemini API
        if GEMINI_AVAILABLE:
            api_key = self._load_gemini_key()
            if api_key:
                try:
                    genai.configure(api_key=api_key)
                    self.gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
                    print("âœ… Gemini API é€£æ¥æˆåŠŸï¼")
                except Exception as e:
                    print(f"âŒ Gemini API åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def _find_gguf_model(self) -> Optional[Path]:
        """åœ¨ LLM è³‡æ–™å¤¾ä¸­å°‹æ‰¾ GGUF æ¨¡å‹"""
        if not self.llm_path.exists():
            print(f"âŒ LLM è³‡æ–™å¤¾ä¸å­˜åœ¨: {self.llm_path}")
            return None
        
        gguf_files = list(self.llm_path.glob("*.gguf"))
        if not gguf_files:
            print(f"âŒ åœ¨ {self.llm_path} ä¸­æ‰¾ä¸åˆ° .gguf æª”æ¡ˆ")
            return None
        
        # å„ªå…ˆé¸æ“‡ Qwen3 æ¨¡å‹
        for file in gguf_files:
            if "qwen3" in file.name.lower():
                return file
        
        return gguf_files[0]  # è¿”å›ç¬¬ä¸€å€‹æ‰¾åˆ°çš„æ¨¡å‹
    
    def _load_gemini_key(self) -> Optional[str]:
        """å¾ .env.local åŠ è¼‰ Gemini API Key"""
        env_file = self.project_root / ".env.local"
        if not env_file.exists():
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {env_file}")
            return None
        
        try:
            with open(env_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.startswith('GEMINI_API_KEY'):
                        key = line.split('=', 1)[1].strip().strip('"').strip("'")
                        return key
        except Exception as e:
            print(f"âŒ è®€å– API Key å¤±æ•—: {e}")
        
        return None
    
    def _should_use_local(self, task_complexity: str, prompt: str) -> bool:
        """
        æ±ºå®šä½¿ç”¨æœ¬åœ°æ¨¡å‹é‚„æ˜¯ Gemini
        
        ç­–ç•¥ï¼š
        - simple: æœ¬åœ°æ¨¡å‹ï¼ˆå¿«é€ŸéŸ¿æ‡‰ï¼‰
        - medium: å„ªå…ˆæœ¬åœ°ï¼Œå¤±æ•—å‰‡ Gemini
        - complex: Geminiï¼ˆæ›´å¼·æ¨ç†èƒ½åŠ›ï¼‰
        """
        if not self.local_model:
            return False
        
        if not self.gemini_model:
            return True
        
        # æ ¹æ“šè¤‡é›œåº¦æ±ºç­–
        if task_complexity == "simple":
            return True
        elif task_complexity == "medium":
            return len(prompt) < 2000  # çŸ­æç¤ºç”¨æœ¬åœ°
        else:  # complex
            return False
    
    def generate(
        self,
        prompt: str,
        task_complexity: str = "medium",
        max_tokens: int = 2048,
        temperature: float = 0.7,
        system_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æ··åˆæ¨ç†ç”Ÿæˆ
        
        Args:
            prompt: ç”¨æˆ¶æç¤º
            task_complexity: "simple", "medium", "complex"
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•¸
            temperature: éš¨æ©Ÿæ€§ï¼ˆ0-1ï¼‰
            system_prompt: ç³»çµ±æç¤º
            
        Returns:
            {
                "text": ç”Ÿæˆçš„æ–‡æœ¬,
                "model": "local" or "gemini",
                "tokens": token æ•¸é‡,
                "time": æ¨ç†æ™‚é–“ï¼ˆç§’ï¼‰
            }
        """
        start_time = datetime.now()
        
        use_local = self._should_use_local(task_complexity, prompt)
        
        try:
            if use_local and self.local_model:
                result = self._generate_local(prompt, max_tokens, temperature, system_prompt)
            elif self.gemini_model:
                result = self._generate_gemini(prompt, max_tokens, temperature, system_prompt)
            else:
                return {
                    "text": "âŒ æ²’æœ‰å¯ç”¨çš„æ¨ç†å¼•æ“",
                    "model": "none",
                    "tokens": 0,
                    "time": 0,
                    "error": "No model available"
                }
            
            # è¨ˆç®—æ™‚é–“
            result["time"] = (datetime.now() - start_time).total_seconds()
            
            # æ›´æ–°çµ±è¨ˆ
            self._update_stats(result)
            
            return result
            
        except Exception as e:
            # å¤±æ•—å›é€€æ©Ÿåˆ¶
            if use_local and self.gemini_model:
                print(f"âš ï¸  æœ¬åœ°æ¨¡å‹å¤±æ•—ï¼Œåˆ‡æ›åˆ° Gemini: {e}")
                return self.generate(prompt, "complex", max_tokens, temperature, system_prompt)
            else:
                return {
                    "text": f"âŒ æ¨ç†å¤±æ•—: {str(e)}",
                    "model": "error",
                    "tokens": 0,
                    "time": 0,
                    "error": str(e)
                }
    
    def _generate_local(
        self,
        prompt: str,
        max_tokens: int,
        temperature: float,
        system_prompt: Optional[str]
    ) -> Dict[str, Any]:
        """ä½¿ç”¨æœ¬åœ° Qwen3 æ¨¡å‹ç”Ÿæˆ"""
        # æ§‹å»ºå®Œæ•´æç¤º
        if system_prompt:
            full_prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
        else:
            full_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
        
        # ç”Ÿæˆ
        response = self.local_model(
            full_prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            stop=["<|im_end|>"],
            echo=False
        )
        
        text = response['choices'][0]['text'].strip()
        tokens = response['usage']['completion_tokens']
        
        return {
            "text": text,
            "model": "local_qwen3",
            "tokens": tokens
        }
    
    def _generate_gemini(
        self,
        prompt: str,
        max_tokens: int,
        temperature: float,
        system_prompt: Optional[str]
    ) -> Dict[str, Any]:
        """ä½¿ç”¨ Gemini API ç”Ÿæˆ"""
        # æ§‹å»ºå®Œæ•´æç¤º
        if system_prompt:
            full_prompt = f"{system_prompt}\n\n{prompt}"
        else:
            full_prompt = prompt
        
        # ç”Ÿæˆé…ç½®
        generation_config = {
            "max_output_tokens": max_tokens,
            "temperature": temperature,
        }
        
        # ç”Ÿæˆ
        response = self.gemini_model.generate_content(
            full_prompt,
            generation_config=generation_config
        )
        
        text = response.text
        tokens = response.usage_metadata.total_token_count if hasattr(response, 'usage_metadata') else 0
        
        return {
            "text": text,
            "model": "gemini",
            "tokens": tokens
        }
    
    def _update_stats(self, result: Dict[str, Any]):
        """æ›´æ–°æ€§èƒ½çµ±è¨ˆ"""
        if result["model"] == "local_qwen3":
            self.stats["local_calls"] += 1
            self.stats["local_tokens"] += result["tokens"]
            # è¨ˆç®—å¹³å‡æ™‚é–“
            n = self.stats["local_calls"]
            self.stats["avg_local_time"] = (
                self.stats["avg_local_time"] * (n - 1) + result["time"]
            ) / n
        elif result["model"] == "gemini":
            self.stats["gemini_calls"] += 1
            self.stats["gemini_tokens"] += result["tokens"]
            n = self.stats["gemini_calls"]
            self.stats["avg_gemini_time"] = (
                self.stats["avg_gemini_time"] * (n - 1) + result["time"]
            ) / n
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–ä½¿ç”¨çµ±è¨ˆ"""
        return self.stats.copy()
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        task_complexity: str = "medium",
        max_tokens: int = 2048,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """
        å°è©±æ¨¡å¼ï¼ˆæ”¯æŒå¤šè¼ªå°è©±ï¼‰
        
        Args:
            messages: [{"role": "user"/"assistant", "content": "..."}]
            task_complexity: ä»»å‹™è¤‡é›œåº¦
            max_tokens: æœ€å¤§ token
            temperature: éš¨æ©Ÿæ€§
            
        Returns:
            ç”Ÿæˆçµæœå­—å…¸
        """
        # æ§‹å»ºæç¤º
        prompt_parts = []
        system_prompt = None
        
        for msg in messages:
            role = msg["role"]
            content = msg["content"]
            
            if role == "system":
                system_prompt = content
            elif role == "user":
                prompt_parts.append(f"User: {content}")
            elif role == "assistant":
                prompt_parts.append(f"Assistant: {content}")
        
        prompt = "\n\n".join(prompt_parts)
        
        return self.generate(prompt, task_complexity, max_tokens, temperature, system_prompt)


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    print("ğŸš€ æ¸¬è©¦æ··åˆ AI ç³»çµ±\n")
    
    # åˆå§‹åŒ–ï¼ˆå‡è¨­åœ¨ YK ç›®éŒ„ä¸‹é‹è¡Œï¼‰
    project_root = Path("C:/Users/YourUser/YK")  # è«‹ä¿®æ”¹ç‚ºå¯¦éš›è·¯å¾‘
    ai = HybridAI(project_root)
    
    # æ¸¬è©¦ç°¡å–®ä»»å‹™ï¼ˆæ‡‰è©²ç”¨æœ¬åœ°æ¨¡å‹ï¼‰
    print("ğŸ“ æ¸¬è©¦ 1: ç°¡å–®è¨ˆç®—")
    result = ai.generate(
        "è¨ˆç®— 15 * 23 = ?",
        task_complexity="simple",
        max_tokens=100
    )
    print(f"  æ¨¡å‹: {result['model']}")
    print(f"  å›ç­”: {result['text']}")
    print(f"  æ™‚é–“: {result['time']:.2f}s\n")
    
    # æ¸¬è©¦è¤‡é›œä»»å‹™ï¼ˆæ‡‰è©²ç”¨ Geminiï¼‰
    print("ğŸ“ æ¸¬è©¦ 2: è¤‡é›œæ¨ç†")
    result = ai.generate(
        "è¨­è¨ˆä¸€å€‹è‡ªæˆ‘é€²åŒ–çš„ AI ç³»çµ±æ¶æ§‹ï¼Œéœ€è¦åŒ…å«è¨˜æ†¶ç³»çµ±ã€æ²™ç›’æ¸¬è©¦ã€ä»£ç¢¼ç”Ÿæˆå’Œæ€§èƒ½è©•ä¼°æ¨¡å¡Šã€‚",
        task_complexity="complex",
        max_tokens=1024
    )
    print(f"  æ¨¡å‹: {result['model']}")
    print(f"  å›ç­”: {result['text'][:200]}...")
    print(f"  æ™‚é–“: {result['time']:.2f}s\n")
    
    # é¡¯ç¤ºçµ±è¨ˆ
    print("ğŸ“Š ä½¿ç”¨çµ±è¨ˆ:")
    stats = ai.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
