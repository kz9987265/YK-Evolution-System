"""
æ²™ç›’åŸ·è¡Œå’Œæ¸¬è©¦å¼•æ“
å®‰å…¨åœ°åŸ·è¡Œã€æ¸¬è©¦å’Œè©•ä¼°æ–°ç”Ÿæˆçš„ä»£ç¢¼
"""

import os
import sys
import subprocess
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime
import json
import tempfile
import shutil
import ast
import time


class CodeValidator:
    """ä»£ç¢¼å®‰å…¨é©—è­‰å™¨"""
    
    # å±éšªæ“ä½œé»‘åå–®
    DANGEROUS_IMPORTS = {
        'os.system', 'subprocess.call', 'subprocess.Popen',
        'eval', 'exec', 'compile', '__import__',
        'shutil.rmtree', 'os.remove', 'os.rmdir',
        'pickle', 'shelve',  # å¯èƒ½çš„ä»£ç¢¼æ³¨å…¥
    }
    
    DANGEROUS_BUILTINS = {
        'eval', 'exec', 'compile', '__import__',
        'open',  # é™åˆ¶æ–‡ä»¶æ“ä½œ
    }
    
    @staticmethod
    def is_safe(code: str) -> Tuple[bool, str]:
        """
        æª¢æŸ¥ä»£ç¢¼æ˜¯å¦å®‰å…¨
        
        Returns:
            (is_safe, reason)
        """
        try:
            # 1. å˜—è©¦è§£æä»£ç¢¼
            tree = ast.parse(code)
        except SyntaxError as e:
            return False, f"èªæ³•éŒ¯èª¤: {e}"
        
        # 2. æª¢æŸ¥å±éšªæ“ä½œ
        for node in ast.walk(tree):
            # æª¢æŸ¥å±éšªçš„å°å…¥
            if isinstance(node, ast.Import):
                for alias in node.names:
                    if any(danger in alias.name for danger in ['os', 'subprocess', 'sys']):
                        return False, f"ç¦æ­¢å°å…¥å±éšªæ¨¡çµ„: {alias.name}"
            
            if isinstance(node, ast.ImportFrom):
                if node.module and any(danger in node.module for danger in ['os', 'subprocess', 'sys']):
                    return False, f"ç¦æ­¢å¾å±éšªæ¨¡çµ„å°å…¥: {node.module}"
            
            # æª¢æŸ¥å±éšªçš„å‡½æ•¸èª¿ç”¨
            if isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name):
                    if node.func.id in CodeValidator.DANGEROUS_BUILTINS:
                        return False, f"ç¦æ­¢ä½¿ç”¨å±éšªå…§å»ºå‡½æ•¸: {node.func.id}"
        
        return True, "ä»£ç¢¼å®‰å…¨"
    
    @staticmethod
    def estimate_complexity(code: str) -> int:
        """ä¼°ç®—ä»£ç¢¼è¤‡é›œåº¦ï¼ˆè¡Œæ•¸ + å‡½æ•¸æ•¸é‡ + é¡æ•¸é‡ï¼‰"""
        try:
            tree = ast.parse(code)
            lines = len(code.split('\n'))
            functions = sum(1 for node in ast.walk(tree) if isinstance(node, ast.FunctionDef))
            classes = sum(1 for node in ast.walk(tree) if isinstance(node, ast.ClassDef))
            
            return lines + functions * 10 + classes * 20
        except:
            return len(code.split('\n'))


class SandboxExecutor:
    """æ²™ç›’ä»£ç¢¼åŸ·è¡Œå™¨"""
    
    def __init__(self, sandbox_root: Path):
        self.sandbox_root = sandbox_root
        self.test_modules_dir = sandbox_root / "test_modules"
        self.benchmarks_dir = sandbox_root / "benchmarks"
        self.evolution_logs_dir = sandbox_root / "evolution_logs"
        
        # å‰µå»ºç›®éŒ„
        for directory in [self.test_modules_dir, self.benchmarks_dir, self.evolution_logs_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        self.validator = CodeValidator()
        
        print("âœ… æ²™ç›’ç’°å¢ƒåˆå§‹åŒ–å®Œæˆ")
    
    def execute_safe(
        self,
        code: str,
        timeout: int = 10,
        globals_dict: Optional[Dict] = None,
        locals_dict: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        å®‰å…¨åŸ·è¡Œä»£ç¢¼
        
        Args:
            code: è¦åŸ·è¡Œçš„ä»£ç¢¼
            timeout: è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
            globals_dict: å…¨å±€è®Šé‡
            locals_dict: å±€éƒ¨è®Šé‡
            
        Returns:
            {
                "success": bool,
                "output": str,
                "error": str,
                "execution_time": float,
                "result": Any
            }
        """
        # 1. å®‰å…¨æª¢æŸ¥
        is_safe, reason = self.validator.is_safe(code)
        if not is_safe:
            return {
                "success": False,
                "output": "",
                "error": f"å®‰å…¨æª¢æŸ¥å¤±æ•—: {reason}",
                "execution_time": 0,
                "result": None
            }
        
        # 2. æº–å‚™åŸ·è¡Œç’°å¢ƒ
        if globals_dict is None:
            globals_dict = {
                "__builtins__": {
                    # åªæä¾›å®‰å…¨çš„å…§å»ºå‡½æ•¸
                    "print": print,
                    "len": len,
                    "range": range,
                    "str": str,
                    "int": int,
                    "float": float,
                    "bool": bool,
                    "list": list,
                    "dict": dict,
                    "tuple": tuple,
                    "set": set,
                    "sum": sum,
                    "max": max,
                    "min": min,
                    "abs": abs,
                    "round": round,
                    "sorted": sorted,
                    "enumerate": enumerate,
                    "zip": zip,
                    "map": map,
                    "filter": filter,
                }
            }
        
        if locals_dict is None:
            locals_dict = {}
        
        # 3. æ•ç²è¼¸å‡º
        from io import StringIO
        import sys
        
        old_stdout = sys.stdout
        old_stderr = sys.stderr
        stdout_capture = StringIO()
        stderr_capture = StringIO()
        
        result = {
            "success": False,
            "output": "",
            "error": "",
            "execution_time": 0,
            "result": None
        }
        
        try:
            sys.stdout = stdout_capture
            sys.stderr = stderr_capture
            
            start_time = time.time()
            
            # åŸ·è¡Œä»£ç¢¼
            exec(code, globals_dict, locals_dict)
            
            end_time = time.time()
            
            result["success"] = True
            result["output"] = stdout_capture.getvalue()
            result["execution_time"] = end_time - start_time
            result["result"] = locals_dict.get('result', None)
            
        except Exception as e:
            result["error"] = f"{type(e).__name__}: {str(e)}\n{traceback.format_exc()}"
        
        finally:
            sys.stdout = old_stdout
            sys.stderr = old_stderr
        
        return result
    
    def test_module(
        self,
        module_code: str,
        test_cases: List[Dict[str, Any]],
        module_name: str = "test_module"
    ) -> Dict[str, Any]:
        """
        æ¸¬è©¦æ¨¡å¡Šä»£ç¢¼
        
        Args:
            module_code: æ¨¡å¡Šä»£ç¢¼
            test_cases: æ¸¬è©¦ç”¨ä¾‹åˆ—è¡¨ [{"input": ..., "expected": ...}, ...]
            module_name: æ¨¡å¡Šåç¨±
            
        Returns:
            {
                "success": bool,
                "passed": int,
                "failed": int,
                "total": int,
                "results": List[Dict],
                "score": float  # 0-1
            }
        """
        # 1. ä¿å­˜æ¨¡å¡Šåˆ°æ²™ç›’
        module_path = self.test_modules_dir / f"{module_name}.py"
        with open(module_path, 'w', encoding='utf-8') as f:
            f.write(module_code)
        
        # 2. é‹è¡Œæ¸¬è©¦ç”¨ä¾‹
        passed = 0
        failed = 0
        test_results = []
        
        for i, test_case in enumerate(test_cases):
            test_input = test_case.get("input", {})
            expected = test_case.get("expected")
            
            # æ§‹å»ºæ¸¬è©¦ä»£ç¢¼
            test_code = f"""
{module_code}

# æ¸¬è©¦ç”¨ä¾‹ {i+1}
test_input = {repr(test_input)}
result = main(**test_input) if callable(main) else None
"""
            
            # åŸ·è¡Œæ¸¬è©¦
            exec_result = self.execute_safe(test_code, timeout=5)
            
            if exec_result["success"]:
                actual = exec_result["result"]
                
                # æ¯”è¼ƒçµæœ
                if actual == expected:
                    passed += 1
                    test_results.append({
                        "test_id": i + 1,
                        "passed": True,
                        "input": test_input,
                        "expected": expected,
                        "actual": actual
                    })
                else:
                    failed += 1
                    test_results.append({
                        "test_id": i + 1,
                        "passed": False,
                        "input": test_input,
                        "expected": expected,
                        "actual": actual,
                        "error": "è¼¸å‡ºä¸ç¬¦åˆé æœŸ"
                    })
            else:
                failed += 1
                test_results.append({
                    "test_id": i + 1,
                    "passed": False,
                    "input": test_input,
                    "error": exec_result["error"]
                })
        
        total = len(test_cases)
        score = passed / total if total > 0 else 0
        
        return {
            "success": score > 0,
            "passed": passed,
            "failed": failed,
            "total": total,
            "results": test_results,
            "score": score
        }
    
    def benchmark(
        self,
        code: str,
        iterations: int = 100,
        warmup: int = 10
    ) -> Dict[str, Any]:
        """
        æ€§èƒ½åŸºæº–æ¸¬è©¦
        
        Args:
            code: è¦æ¸¬è©¦çš„ä»£ç¢¼
            iterations: è¿­ä»£æ¬¡æ•¸
            warmup: é ç†±æ¬¡æ•¸
            
        Returns:
            {
                "avg_time": float,
                "min_time": float,
                "max_time": float,
                "total_time": float
            }
        """
        times = []
        
        # é ç†±
        for _ in range(warmup):
            self.execute_safe(code, timeout=5)
        
        # æ­£å¼æ¸¬è©¦
        for _ in range(iterations):
            result = self.execute_safe(code, timeout=5)
            if result["success"]:
                times.append(result["execution_time"])
        
        if not times:
            return {
                "avg_time": 0,
                "min_time": 0,
                "max_time": 0,
                "total_time": 0,
                "error": "æ‰€æœ‰è¿­ä»£éƒ½å¤±æ•—äº†"
            }
        
        return {
            "avg_time": sum(times) / len(times),
            "min_time": min(times),
            "max_time": max(times),
            "total_time": sum(times),
            "iterations": len(times)
        }
    
    def compare_versions(
        self,
        old_code: str,
        new_code: str,
        test_cases: List[Dict[str, Any]],
        benchmark_iterations: int = 50
    ) -> Dict[str, Any]:
        """
        æ¯”è¼ƒå…©å€‹ç‰ˆæœ¬çš„ä»£ç¢¼
        
        Returns:
            {
                "old_score": float,
                "new_score": float,
                "improvement": float,
                "old_performance": Dict,
                "new_performance": Dict,
                "recommendation": str
            }
        """
        # 1. åŠŸèƒ½æ¸¬è©¦
        old_test = self.test_module(old_code, test_cases, "old_version")
        new_test = self.test_module(new_code, test_cases, "new_version")
        
        # 2. æ€§èƒ½æ¸¬è©¦
        old_perf = self.benchmark(old_code, benchmark_iterations)
        new_perf = self.benchmark(new_code, benchmark_iterations)
        
        # 3. è¨ˆç®—æ”¹é€²
        score_improvement = new_test["score"] - old_test["score"]
        
        if old_perf.get("avg_time", 0) > 0:
            perf_improvement = (old_perf["avg_time"] - new_perf.get("avg_time", 0)) / old_perf["avg_time"]
        else:
            perf_improvement = 0
        
        # 4. ç¶œåˆè©•ä¼°
        total_improvement = score_improvement * 0.7 + perf_improvement * 0.3
        
        # 5. æ¨è–¦
        if total_improvement > 0.1:
            recommendation = "âœ… æ–°ç‰ˆæœ¬æ˜é¡¯æ›´å¥½ï¼Œå»ºè­°æ¡ç”¨"
        elif total_improvement > 0:
            recommendation = "âš ï¸  æ–°ç‰ˆæœ¬ç•¥æœ‰æ”¹é€²ï¼Œå¯ä»¥è€ƒæ…®æ¡ç”¨"
        elif total_improvement > -0.05:
            recommendation = "âš–ï¸  æ–°èˆŠç‰ˆæœ¬å·®ä¸å¤šï¼Œä¿ç•™èˆŠç‰ˆæœ¬"
        else:
            recommendation = "âŒ æ–°ç‰ˆæœ¬æ›´å·®ï¼Œæ‹’çµ•æ¡ç”¨"
        
        return {
            "old_score": old_test["score"],
            "new_score": new_test["score"],
            "score_improvement": score_improvement,
            "old_performance": old_perf,
            "new_performance": new_perf,
            "performance_improvement": perf_improvement,
            "total_improvement": total_improvement,
            "recommendation": recommendation,
            "old_test_results": old_test,
            "new_test_results": new_test
        }
    
    def log_evolution(
        self,
        module_name: str,
        old_code: str,
        new_code: str,
        comparison: Dict[str, Any],
        accepted: bool
    ):
        """è¨˜éŒ„é€²åŒ–æ—¥èªŒ"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "module_name": module_name,
            "old_complexity": self.validator.estimate_complexity(old_code),
            "new_complexity": self.validator.estimate_complexity(new_code),
            "score_improvement": comparison["score_improvement"],
            "performance_improvement": comparison["performance_improvement"],
            "total_improvement": comparison["total_improvement"],
            "recommendation": comparison["recommendation"],
            "accepted": accepted,
            "old_code_hash": hash(old_code),
            "new_code_hash": hash(new_code)
        }
        
        # ä¿å­˜æ—¥èªŒ
        log_file = self.evolution_logs_dir / f"evolution_{datetime.now().strftime('%Y%m%d')}.jsonl"
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
        
        return log_entry
    
    def cleanup(self):
        """æ¸…ç†è‡¨æ™‚æ–‡ä»¶"""
        for file in self.test_modules_dir.glob("*.py"):
            if file.name.startswith("test_") or file.name.startswith("old_") or file.name.startswith("new_"):
                file.unlink()


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    print("ğŸ§ª æ¸¬è©¦æ²™ç›’åŸ·è¡Œå¼•æ“\n")
    
    sandbox_root = Path("C:/Users/YourUser/YK/Sandbox")  # è«‹ä¿®æ”¹è·¯å¾‘
    executor = SandboxExecutor(sandbox_root)
    
    # æ¸¬è©¦ 1: å®‰å…¨åŸ·è¡Œ
    print("ğŸ“ æ¸¬è©¦ 1: å®‰å…¨åŸ·è¡Œä»£ç¢¼")
    safe_code = """
result = sum([1, 2, 3, 4, 5])
print(f"ç¸½å’Œ: {result}")
"""
    result = executor.execute_safe(safe_code)
    print(f"  æˆåŠŸ: {result['success']}")
    print(f"  è¼¸å‡º: {result['output']}")
    print(f"  çµæœ: {result['result']}")
    print(f"  æ™‚é–“: {result['execution_time']:.4f}s\n")
    
    # æ¸¬è©¦ 2: å±éšªä»£ç¢¼æª¢æ¸¬
    print("ğŸ“ æ¸¬è©¦ 2: å±éšªä»£ç¢¼æª¢æ¸¬")
    dangerous_code = "import os\nos.system('rm -rf /')"
    result = executor.execute_safe(dangerous_code)
    print(f"  æˆåŠŸ: {result['success']}")
    print(f"  éŒ¯èª¤: {result['error']}\n")
    
    # æ¸¬è©¦ 3: æ¨¡å¡Šæ¸¬è©¦
    print("ğŸ“ æ¸¬è©¦ 3: æ¨¡å¡ŠåŠŸèƒ½æ¸¬è©¦")
    module_code = """
def main(a, b):
    return a + b
"""
    test_cases = [
        {"input": {"a": 1, "b": 2}, "expected": 3},
        {"input": {"a": 10, "b": 20}, "expected": 30},
        {"input": {"a": -5, "b": 5}, "expected": 0},
    ]
    
    test_result = executor.test_module(module_code, test_cases)
    print(f"  é€šé: {test_result['passed']}/{test_result['total']}")
    print(f"  åˆ†æ•¸: {test_result['score']:.2%}\n")
    
    # æ¸¬è©¦ 4: æ€§èƒ½åŸºæº–
    print("ğŸ“ æ¸¬è©¦ 4: æ€§èƒ½åŸºæº–æ¸¬è©¦")
    perf_result = executor.benchmark("result = sum(range(1000))", iterations=100)
    print(f"  å¹³å‡æ™‚é–“: {perf_result['avg_time']*1000:.3f}ms")
    print(f"  æœ€å°æ™‚é–“: {perf_result['min_time']*1000:.3f}ms")
    print(f"  æœ€å¤§æ™‚é–“: {perf_result['max_time']*1000:.3f}ms\n")
    
    # æ¸¬è©¦ 5: ç‰ˆæœ¬æ¯”è¼ƒ
    print("ğŸ“ æ¸¬è©¦ 5: ç‰ˆæœ¬æ¯”è¼ƒ")
    old_code = """
def main(n):
    result = 0
    for i in range(n):
        result += i
    return result
"""
    new_code = """
def main(n):
    return sum(range(n))
"""
    
    comparison = executor.compare_versions(
        old_code, new_code,
        test_cases=[
            {"input": {"n": 10}, "expected": 45},
            {"input": {"n": 100}, "expected": 4950},
        ]
    )
    
    print(f"  èˆŠç‰ˆæœ¬åˆ†æ•¸: {comparison['old_score']:.2%}")
    print(f"  æ–°ç‰ˆæœ¬åˆ†æ•¸: {comparison['new_score']:.2%}")
    print(f"  æ€§èƒ½æ”¹é€²: {comparison['performance_improvement']:.2%}")
    print(f"  æ¨è–¦: {comparison['recommendation']}\n")
    
    print("âœ… æ¸¬è©¦å®Œæˆï¼")
